---
title: Lauterbrunnen
date: 2023-09-07
publishdate: 2023-09-07
draft: true
---

If you use Kafka to connect your microservices, sooner or later you start thinking about how to ensure that **your data** (stored in some database) and **your events** (the ones you publish in Kafka to notify anyone else about your data changes) tell the same story.


Recently I've been talking to someone that faced this problem. Instead of going through the different alternatives (this [article](https://blog.devgenius.io/transactional-integration-kafka-with-database-7eb5fc270bdc) is a nice reference) I'll try to recap the key points (in my experience).


- Use **[transactional outbox](https://microservices.io/patterns/data/transactional-outbox.html) pattern**. There are many different ways to integrate a non-transactional resource (like Kafka) into a transaction but I wouldn't go that way.

- It's not easy to get **strict** ordering in a performant way from the outbox table. Think of a SORT BY criteria that reflects transaction order (you get the rows in the same order that you'll get those INSERTs if inspecting the transactional log). I couldn't think of any but inspecting the transactional log. So ....

- **Don't extract your events through SQL** (i.e. [Kafka JDBC Connector](https://docs.confluent.io/kafka-connectors/jdbc/current/index.html)). 

- Prefer the **transactional log** (or equivalent). If your database is supported, you can use an out-of-box solution like [Debezium](https://debezium.io/). In MySQL (or MariaDB) you can leverage [blackhole](https://mariadb.com/kb/en/blackhole/) storage so that you don't need to take care of cleaning up your outbox table.


