<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Works - loteroc@gmail.com on maqroll personal site</title>
    <link>https://maqroll.github.io/</link>
    <description>Recent content in Works - loteroc@gmail.com on maqroll personal site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 17 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://maqroll.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Processing Kafka in Vert.x with control flow</title>
      <link>https://maqroll.github.io/notes/post-20220917/</link>
      <pubDate>Sat, 17 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/notes/post-20220917/</guid>
      <description>When you read asynchronously from Kafka in Vert.x (subscribing to a KafkaConsumer) ingestion rate it&amp;rsquo;s capped basically by the Kafka broker and the network bandwidth (take this as a simplistic statement). What I really mean it&amp;rsquo;s that this setup can achieve really high ingestion rates without much effort.
If you opt to process those messages synchronously (providing that you have a KafkaConsumer per partition) your performance will be roughly that of Kafka Streams.</description>
    </item>
    
    <item>
      <title>Rockset</title>
      <link>https://maqroll.github.io/notes/post-20220710/</link>
      <pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/notes/post-20220710/</guid>
      <description>If you do stateful processing in Kafka Streams, you have already heard of Rocksdb. Rocksdb is a persistent key value store focused on performance. Despite its terse functionality set (put/get/range get and prefix get) is a powerful tool.
One of the members of the original team that developed Rocksdb at facebook founded Rockset. Rockset indexes automatically structured information (json, avro, etc..) on top of Rocksdb and allows to query that info using SQL.</description>
    </item>
    
    <item>
      <title>InMemoryKeyValueStore without serialization/deserialization</title>
      <link>https://maqroll.github.io/notes/post-20220124/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/notes/post-20220124/</guid>
      <description>KStreams provides an in-memory key value store (backed by a topic).
For instance:
 builder.addGlobalStore( Stores.keyValueStoreBuilder( Stores.inMemoryKeyValueStore(STORES), Serdes.String(), Serdes.String()), STORES_TOPIC, Consumed.with(Serdes.String(), Serdes.String()), () -&amp;gt; new GlobalStoreUpdater&amp;lt;&amp;gt;(STORES)); declares one backed by STORES_TOPIC and called STORES. Both key and value are strings.
Being in-memory, heap is a limit to the size of the store.
We use it mostly to implement global stores for master data. Any other system keeps master data updated in the topic and we consume it directly into the global store as a convenient way to make key-value searches.</description>
    </item>
    
    <item>
      <title>Hacking Kafka Connect</title>
      <link>https://maqroll.github.io/notes/post-20211104/</link>
      <pubDate>Thu, 04 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/notes/post-20211104/</guid>
      <description>Second time we face the same problem: process Avro messages with Kafka Connect honoring a schema already setup in Schema Registry.
It looks like the typical interview problem you&amp;rsquo;ll never find in real life. But you know, it&amp;rsquo;s sitting on the sofa.
The interest of this note (if any) is not providing an answer but thinking about why we solved it differently the second time.
 Kafka Connect is a generic framework for processing Kafka messages.</description>
    </item>
    
    <item>
      <title>Nice to meet you, RandomBeans!</title>
      <link>https://maqroll.github.io/notes/post-20211027/</link>
      <pubDate>Wed, 27 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/notes/post-20211027/</guid>
      <description>Hand-coding complex domain entities (with many nested levels) for unit testing is a major pain point.
Taking Avro into consideration make the problem worse (polymorphic union types).
Discovering RandomBeans extension it&amp;rsquo;s been a relief.
Recommended!.</description>
    </item>
    
    <item>
      <title>Nice to meet you, Vert.x!</title>
      <link>https://maqroll.github.io/notes/post-20211024/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/notes/post-20211024/</guid>
      <description>Almost a year ago we decided to give Eclipse Vert.x an opportunity for a new product we were cooking.
The product involves generating NRT indicators over several streams of data coming from Kafka. Volume is high in general but it has to cope with peak retail sales (BF, sales season, etc).
Usually we&amp;rsquo;d turn to KStreams for this kind of processing. To reduce latency, in this product we decided to serve the indicators directly to consumers over websockets.</description>
    </item>
    
    <item>
      <title>BePP</title>
      <link>https://maqroll.github.io/works/bepp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/works/bepp/</guid>
      <description> BePP Most impactful project so far.
Between 2002 and 2003 I led a great team that designed a new architecture for home banking applications for a major global brand.
Three installations were made in Portugal. After successive mergers, the only one that survives is netbanco.
As of 2020/09/01 is alive and (seems to be) using the same platform.
   </description>
    </item>
    
    <item>
      <title>Metropolis</title>
      <link>https://maqroll.github.io/works/metropolis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/works/metropolis/</guid>
      <description> Metropolis Around 2008 we designed a new core-banking architecture for a major bank in Spain codenamed &#34;Metropolis&#34;.
That technology later became CECA&#39;s reference architecture.
Still in use.
   </description>
    </item>
    
    <item>
      <title>NRT sales funnel</title>
      <link>https://maqroll.github.io/works/sales/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/works/sales/</guid>
      <description> NRT sales funnel Imagine the look&amp;feel of funnel-graph-js but updated in near-realtime (less than 5 seconds lag) segmented by country, device type, etc...
You got it!
Besides: top N products in each step (visits, carts and orders) filterable for different criteria(country, section, etc..).
Like this one but aggregating thousands of messages per second under multiple criteria
Sure you&#39;ll miss something like this next BF.
   </description>
    </item>
    
    <item>
      <title>password()</title>
      <link>https://maqroll.github.io/works/passfn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/works/passfn/</guid>
      <description> password() Instead of storing your passwords calculate them using a secure hash function conveniently.
oneshallpass.com reincarnated as a tampermonkey script available through a hotkey.
   </description>
    </item>
    
    <item>
      <title>rc4cygwin</title>
      <link>https://maqroll.github.io/works/rc4cygwin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://maqroll.github.io/works/rc4cygwin/</guid>
      <description> rc shell for cygwin rc was built from scratch for plan9 operating system.
Simple and powerful, I was very fond of it. Code is short enough to be manageable and good enough to learn from it.
At that time I used cygwin so patched the original code to work in cygwin. rc package available in cygwin was a rewrite.
I haven&#39;t used it for ages.
   </description>
    </item>
    
  </channel>
</rss>
